{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cells setups the environment when executed in Google Colab.\n",
    "try:\n",
    "    import google.colab\n",
    "    !curl -s https://raw.githubusercontent.com/ibs-lab/cedalion/dev/scripts/colab_setup.py -o colab_setup.py\n",
    "    # Select branch with --branch \"branch name\" (default is \"dev\")\n",
    "    !pip cache purge\n",
    "    %run colab_setup.py --branch \"dev\"\n",
    "\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert a fNIRS dataset to BIDS\n",
    "\n",
    "### Do not run all cells at once. Carefully read the comments before each code cell â€” some steps require you to manually modify certain files (e.g., the mapping CSV) before proceeding. Make sure all required edits are completed before continuing to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import snirf2bids as s2b\n",
    "from pathlib import Path\n",
    "from cedalion.io import bids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide file paths\n",
    "\n",
    "This notebook shows how to convert an fNIRS dataset into BIDS format. To use it, provide the following inputs:\n",
    "\n",
    "1.  Dataset Path: Folder containing the raw dataset.\n",
    "2.  Destination Path: Folder where the BIDS-compliant dataset will be saved.\n",
    "3.  Mapping CSV File: CSV file that defines the dataset structure and provides necessary details for BIDS conversion.\n",
    "4.  (Optional) extra_meta_data File: Additional metadata to include in the description.json file. You can use [this google form](https://docs.google.com/forms/d/e/1FAIpQLSeZjlgIqCwp054HsHmTBKPziqcOlfTcaWpdXcGFYPDf0Q5vNg/viewform?usp=sf_link) or [this website](https://neurojson.org/Create/dataset_description_fnirs) to create this file.\n",
    "5.  (Optional) participants.tsv / participants.json files. If you already have a participants.tsv/.json file and provide the link below, it will be used directly.\n",
    "Alternatively, if you have participant-level metadata saved in a CSV or Excel file, with the first column as the participant ID and the remaining columns as metadata (with appropriate headers) and you provide the link to it below, the script will convert it into properly formatted .tsv and .json files for BIDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'path-to-your-dataset-folder'  # REQUIRED\n",
    "destination_path = 'path-to-your-destination-bids-folder' # REQUIRED\n",
    "\n",
    "dataset_path = Path(dataset_path)\n",
    "destination_path = Path(destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_meta_data_path = Path('path-to-your-meta-data')  # OPTIONAL\n",
    "extra_meta_data_path = extra_meta_data_path if os.path.exists(extra_meta_data_path) else None\n",
    "\n",
    "mapping_df_path = bids.get_snirf2bids_mapping_csv(dataset_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_tsv_file = 'path-to-your-participants.tsv' # OPTIONAL\n",
    "participants_json_file = 'path-to-your-participants.json' # OPTIONAL\n",
    "\n",
    "participants_tsv_file = Path(participants_tsv_file)\n",
    "participants_json_file = Path(participants_json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please modify the mapping CSV file which is automatically created under you raw dataset folder.\n",
    "\n",
    "By default, a mapping CSV file is generated under the main raw dataset folder using the get_snirf2bids_mapping_csv function.\n",
    "Before running the rest of the code, open this file, make any necessary edits, and save it. A valid mapping CSV must include all SNIRF files in your dataset, along with the following columns:\n",
    "\n",
    "- sub: Participant identifier\n",
    "- ses (optional): Session identifier\n",
    "- task: Task name or label\n",
    "- run (optional): Run number\n",
    "- acq (optional): Acquisition label\n",
    "- cond (optional): List of condition labels\n",
    "- cond_match (optional): List of matching condition values\n",
    "- duration (optional): Event duration in seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_df = pd.read_csv(mapping_df_path, dtype=str)\n",
    "mapping_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapping table created below serves as a key component for organizing and processing your dataset. The `ses`, `run`, and `acq` columns are optional and can be set to None if not applicable. The `current_name` column contains the path to the SNIRF files in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for possible *_scan.tsv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure no important information (e.g., acquisition time) from the original dataset is lost, we will:\n",
    "\n",
    "- Search Subdirectories: Traverse through all subdirectories within the dataset.\n",
    "- Locate Existing Scan Files: Search for all *_scan.tsv files in the dataset.\n",
    "- Integrate into Mapping Table: Extract the relevant information from these files and add it to our mapping table.\n",
    "- Extracts acquisition time from SNIRF files if missing in the `_scans.tsv` file.\n",
    "\n",
    "This approach ensures that any details, such as acquisition time, are retained and incorporated into the BIDS-compliant structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_df[\"filename_org\"] = mapping_df[\"current_name\"].apply(\n",
    "    lambda x: os.path.basename(x))\n",
    "scan_df = bids.search_for_acq_time_in_scan_files(dataset_path)\n",
    "\n",
    "mapping_df = pd.merge(mapping_df, scan_df, on=\"filename_org\", how=\"left\")\n",
    "mapping_df[\"acq_time\"] = mapping_df.apply(bids.search_for_acq_time_in_snirf_files, axis=1, args=(dataset_path,))\n",
    "\n",
    "mapping_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `acq_time` information is retrieved from the original dataset's *_scan.tsv files and integrated into the mapping table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for possible *_session.tsv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to *_scan.tsv files, we search for *_session.tsv files in the dataset path to capture additional session-level metadata, such as acquisition times. Any relevant information from these files is added to the mapping table to ensure all session details are preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_df = bids.search_for_sessions_acq_time(dataset_path)\n",
    "mapping_df = pd.merge(mapping_df, session_df, on=[\"sub\", \"ses\"], how=\"left\")\n",
    "\n",
    "mapping_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BIDS Folder Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this section is to rename the SNIRF files according to the BIDS naming convention and place them in the appropriate directory under `destination_path`, following the BIDS folder structure.\n",
    "\n",
    "Steps:\n",
    "1. Generate new filenames: Create BIDS-compliant filenames for all SNIRF records.\n",
    "2. Determine file locations: Identify the appropriate locations for these files within the BIDS folder hierarchy.\n",
    "\n",
    "This process ensures that the dataset adheres to BIDS standards for organization and naming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_df[[\"bids_name\", \"parent_path\"]] = mapping_df.apply(\n",
    "    bids.create_bids_standard_filenames, axis=1, result_type='expand')\n",
    "\n",
    "mapping_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate proper organization:\n",
    "\n",
    "- `parent_path`: Added to the mapping dataframe to define the location of each SNIRF file within `destination_path`.\n",
    "- `bids_name`: Specifies the new BIDS-compliant name for each file.\n",
    "In the following sections, we will rename all files to their corresponding `bids_name` and copy them to their designated parent_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = mapping_df.apply(bids.copy_rename_snirf, axis=1, args=(dataset_path, destination_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BIDS specific files (e.g., _coordsystem.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we utilize the snirf2bids Python package to generate the necessary .tsv and .json files for the BIDS structure.\n",
    "\n",
    "For every record, the following files will be created:\n",
    "1. _coordsystem.json\n",
    "2. _optodes.json\n",
    "3. _optodes.tsv\n",
    "4. *_channels.tsv\n",
    "5. *_events.json\n",
    "6. *_events.tsv\n",
    "7. *_nirs.json\n",
    "\n",
    "These files are essential for ensuring the dataset adheres to BIDS standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2b.snirf2bids_recurse(destination_path)\n",
    "pattern = re.compile(r'.*_scans\\.tsv$|^participants\\.tsv$|^temp_participants\\.tsv$')\n",
    "files_to_delete = [file for file in destination_path.rglob('*') if file.is_file() and pattern.match(file.name)]\n",
    "for file in files_to_delete:\n",
    "    file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create _scan.tsv Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we proceed to create scan files for all subjects and sessions. Previously, we searched the original dataset path for any provided scan information, which will now be incorporated into the BIDS structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_df = mapping_df[[\"sub\", \"ses\", \"bids_name\", \"acq_time\"]]\n",
    "scan_df['ses'].fillna(\"Unknown\", inplace=True)\n",
    "scan_df = scan_df.groupby([\"sub\", \"ses\"])\n",
    "scan_df.apply(lambda group: bids.create_scan_files(group, destination_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create _session.tsv Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create session files for all subjects. As with the scan files, we previously searched the original dataset path for any session information, which will now be used to create the corresponding BIDS session files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_df = mapping_df[[\"sub\", \"ses\", \"ses_acq_time\"]]\n",
    "session_df = session_df.groupby([\"sub\"])\n",
    "session_df.apply(lambda group: bids.create_session_files(group, destination_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Integrate participants.tsv and participants.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we gather available participant information and incorporate it into the BIDS structure. \n",
    "\n",
    "If you want to use custom participant metadata, you should provide it at the beginning of the code, either as a participants.tsv file or as a CSV/Excel file.\n",
    "\n",
    "- If you provide a participants.tsv file but not a corresponding participants.json, you should fill out the participants.json manually to include descriptions for each field to comply with BIDS standards.\n",
    "\n",
    "- If you provide neither file, new participants.tsv and participants.json files will be automatically created with standard fields:\n",
    "\n",
    "    - species\n",
    "    - age\n",
    "    - sex\n",
    "    - handedness\n",
    "\n",
    "You can also pass your favourite/custom fields instead of these defaults when creating new files (only applies if no valid TSV is provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_participants = bids.create_participants_files(bids_dir=destination_path, \n",
    "                                                    participants_tsv_path= participants_tsv_file, \n",
    "                                                    participants_json_path=participants_json_file, \n",
    "                                                    mapping_df=mapping_df,\n",
    "                                                    fields=[\"gender\", \"age\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data description file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the dataset_description.json file, we follow these steps:\n",
    "\n",
    "1. Search for an existing dataset_description.json in the dataset path and retain the provided information.\n",
    "2. If extra_meta_data_path is specified, add the additional metadata about the dataset.\n",
    "3. If neither dataset_description.json nor extra metadata is provided, use the basename of the dataset directory as the dataset name and set the BIDS version to '1.10.0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids.create_data_description(dataset_path, destination_path, extra_meta_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check _coordsystem.json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since an empty string is not allowed for the `NIRSCoordinateSystem` key in the *_coordsystem.json file, we will populate it with \"Other\" to ensure BIDS compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids.check_coord_files(destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix *_events.tsv order\n",
    "\n",
    "Sorting events files based on onset time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = mapping_df.apply(bids.sort_events, axis=1, args=(destination_path,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit *_events.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow editing of the `duration` or `trial_type` columns in the *_events.tsv files, the mapping CSV file must include the following extra columns:\n",
    "\n",
    "1. `duration`: Specifies the new duration for each SNIRF file that needs editing.\n",
    "2. cond and cond_match:\n",
    "\n",
    "    - cond: A list of existing condition labels found in the SNIRF file (e.g., [1, 2]).\n",
    "\n",
    "    - cond_match: A list of new labels you want to use in place of those conditions (e.g., [\"con\", \"inc\"]).\n",
    "    \n",
    "These two columns will be combined into a dictionary to update the trial_type column in the events file. This allows for relabeling of condition names in a BIDS-compliant way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = mapping_df.apply(bids.edit_events, axis=1, args=(destination_path,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating sourcedata Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally there is this possiblity to keep your original data under sourcedata directory at your `destination_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids.save_source(dataset_path, destination_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cedalion_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
