{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from cedalion.io import bids\n",
    "import snirf2bids as s2b\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert a fNIRS dataset to BIDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook automates the conversion of an fNIRS dataset into a BIDS-compliant format. \n",
    "To begin, you'll need to:\n",
    "\n",
    "1. Dataset Path: The folder containing the raw dataset.\n",
    "2. Destination Path: The folder where the BIDS-compliant files will be saved.\n",
    "3. Mapping CSV File: A CSV file that defines the dataset folder structure and includes the necessary details for constructing the BIDS structure.\n",
    "4. Optional Metadata: Any additional metadata you want to include. You can use [this google form](https://docs.google.com/forms/d/e/1FAIpQLSeZjlgIqCwp054HsHmTBKPziqcOlfTcaWpdXcGFYPDf0Q5vNg/viewform?usp=sf_link) or [this website](https://neurojson.org/Create/dataset_description_fnirs) to create this json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'path-to-your-dataset'\n",
    "destination_path = 'your-destination-bids-path'\n",
    "\n",
    "dataset_path = Path(dataset_path)\n",
    "destination_path = Path(destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_df_path = bids.get_snirf2bids_mapping_csv(dataset_path)\n",
    "extra_meta_data_path = Path(\"path-to-you-meta-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_snirf2bids_mapping_csv` helps you create your mapping CSV file. After generating the CSV file, you might need to manually edit it to include additional information or make adjustments as required.\n",
    "\n",
    "A valid mapping CSV must include all SNIRF files in your dataset, along with the following details for each file:\n",
    "\n",
    "- sub: The identifier for the participant.\n",
    "- ses (optional): The session identifier, if applicable.\n",
    "- task: The task name or label.\n",
    "- run (optional): The run number, if applicable.\n",
    "- acq (optional): The acquisition number, if applicable.\n",
    "- cond (optional): Conditions' keys as a list.\n",
    "- cond_match (optional): Conditions' values as a list.\n",
    "- duration (optional): Events' duration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_df = pd.read_csv(mapping_df_path, dtype=str)\n",
    "mapping_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapping table serves as a key component for organizing and processing your dataset. \n",
    "\n",
    "The `ses`, `run`, and `acq` columns are optional and can be set to None if not applicable. The `current_name` column contains the path to the SNIRF files in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for possible *_scan.tsv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure no important information (e.g., acquisition time) from the original dataset is lost, we will:\n",
    "\n",
    "- Search Subdirectories: Traverse through all subdirectories within the dataset.\n",
    "- Locate Existing Scan Files: Search for all *_scan.tsv files in the dataset.\n",
    "- Integrate into Mapping Table: Extract the relevant information from these files and add it to our mapping table.\n",
    "\n",
    "This approach ensures that any details, such as acquisition time, are retained and incorporated into the BIDS-compliant structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_df[\"filename_org\"] = mapping_df[\"current_name\"].apply(\n",
    "    lambda x: os.path.basename(x))\n",
    "scan_df = bids.search_for_acq_time(dataset_path)\n",
    "mapping_df = pd.merge(mapping_df, scan_df, on=\"filename_org\", how=\"left\")\n",
    "\n",
    "mapping_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `acq_time` information is retrieved from the original dataset's *_scan.tsv files and integrated into the mapping table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for possible *_session.tsv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to *_scan.tsv files, we search for *_session.tsv files in the dataset path to capture additional session-level metadata, such as acquisition times. Any relevant information from these files is added to the mapping table to ensure all session details are preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_df = bids.search_for_sessions_acq_time(dataset_path)\n",
    "mapping_df = pd.merge(mapping_df, session_df, on=[\"sub\", \"ses\"], how=\"left\")\n",
    "\n",
    "mapping_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BIDS Folder Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this section is to rename the SNIRF files according to the BIDS naming convention and place them in the appropriate directory under `destination_path`, following the BIDS folder structure.\n",
    "\n",
    "Steps:\n",
    "1. Generate New Filenames: Create BIDS-compliant filenames for all SNIRF records.\n",
    "2. Determine File Locations: Identify the appropriate locations for these files within the BIDS folder hierarchy.\n",
    "\n",
    "This process ensures that the dataset adheres to BIDS standards for organization and naming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_df[[\"bids_name\", \"parent_path\"]] = mapping_df.apply(\n",
    "    bids.create_bids_standard_filenames, axis=1, result_type='expand')\n",
    "\n",
    "mapping_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate proper organization:\n",
    "\n",
    "- `parent_path`: Added to the mapping dataframe to define the location of each SNIRF file within `destination_path`.\n",
    "- `bids_name`: Specifies the new BIDS-compliant name for each file.\n",
    "In the following sections, we will rename all files to their corresponding `bids_name` and copy them to their designated parent_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = mapping_df.apply(bids.copy_rename_snirf, axis=1, args=(dataset_path, destination_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BIDS specific files (e.g., _coordsystem.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we utilize the snirf2bids Python package to generate the necessary .tsv and .json files for the BIDS structure.\n",
    "\n",
    "For every record, the following files will be created:\n",
    "1. _coordsystem.json\n",
    "2. _optodes.json\n",
    "3. _optodes.tsv\n",
    "4. *_channels.tsv\n",
    "5. *_events.json\n",
    "6. *_events.tsv\n",
    "7. *_nirs.json\n",
    "\n",
    "These files are essential for ensuring the dataset adheres to BIDS standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2b.snirf2bids_recurse(destination_path)\n",
    "pattern = re.compile(r'.*_scans\\.tsv$|^participants\\.tsv$|^temp_participants\\.tsv$')\n",
    "files_to_delete = [file for file in destination_path.rglob('*') if file.is_file() and pattern.match(file.name)]\n",
    "for file in files_to_delete:\n",
    "    file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create _scan.tsv Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we proceed to create scan files for all subjects and sessions. Previously, we searched the original dataset path for any provided scan information, which will now be incorporated into the BIDS structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_df = mapping_df[[\"sub\", \"ses\", \"bids_name\", \"acq_time\"]]\n",
    "scan_df['ses'].fillna(\"Unknown\", inplace=True)\n",
    "scan_df = scan_df.groupby([\"sub\", \"ses\"])\n",
    "scan_df.apply(lambda group: bids.create_scan_files(group, destination_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create _session.tsv Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create session files for all subjects. As with the scan files, we previously searched the original dataset path for any session information, which will now be used to create the corresponding BIDS session files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_df = mapping_df[[\"sub\", \"ses\", \"ses_acq_time\"]]\n",
    "session_df = session_df.groupby([\"sub\"])\n",
    "session_df.apply(lambda group: bids.create_session_files(group, destination_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create participants tsv and json files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we gather all available participant information from the original dataset. If any participant details are provided, they will be incorporated into the BIDS structure.\n",
    "\n",
    "Additionally, we create a template for the participants.json file with predefined columns, including:\n",
    "\n",
    "- species\n",
    "- age\n",
    "- sex\n",
    "- handedness\n",
    "\n",
    "Each of these fields will include descriptive templates to ensure consistency in the BIDS-compliant structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids.create_participants_tsv(dataset_path, destination_path, mapping_df)\n",
    "bids.create_participants_json(dataset_path, destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data description file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the dataset_description.json file, we follow these steps:\n",
    "\n",
    "1. Search for an existing dataset_description.json in the dataset path and retain the provided information.\n",
    "2. If extra_meta_data_path is specified, add the additional metadata about the dataset.\n",
    "3. If neither dataset_description.json nor extra metadata is provided, use the basename of the dataset directory as the dataset name and set the BIDS version to '1.10.0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids.create_data_description(dataset_path, destination_path, extra_meta_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check _coordsystem.json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since an empty string is not allowed for the `NIRSCoordinateSystem` key in the *_coordsystem.json file, we will populate it with \"Other\" to ensure BIDS compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids.check_coord_files(destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit *_events.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow editing of the `duration` or `trial_type` columns in the *_events.tsv files, the mapping CSV file must include the following extra columns:\n",
    "\n",
    "1. `duration`: Specifies the new duration for each SNIRF file that needs editing.\n",
    "2. cond and cond_match:\n",
    "    - `cond`: A list of keys e.g. [1, 2].\n",
    "    - `cond_match`: A list of corresponding values e.g. [\"con\", \"inc\"]. \n",
    "    \n",
    "    These two columns will be used to create a dictionary that maps the trial_type column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = mapping_df.apply(bids.edit_events, axis=1, args=(destination_path,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more extensive *_events.tsv files, you can use [this](https://docs.google.com/forms/d/e/1FAIpQLSeZjlgIqCwp054HsHmTBKPziqcOlfTcaWpdXcGFYPDf0Q5vNg/viewform?usp=sf_link) google form or https://neurojson.io/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating sourcedata Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally there is this possiblity to keep your original data under sourcedata directory at your `destination_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids.save_source(dataset_path, destination_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cedalion_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
