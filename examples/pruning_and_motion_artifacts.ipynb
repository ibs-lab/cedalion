{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel  Quality Assessment, Pruning, and Motion Artifact Detection\n",
    "\n",
    "This notebook sketches how to prune bad channels and detect motion artefacts in fNIRS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cedalion\n",
    "import cedalion.nirs\n",
    "import cedalion.sigproc.quality as quality\n",
    "import cedalion.xrutils as xrutils\n",
    "import cedalion.datasets as datasets\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as p\n",
    "\n",
    "from cedalion import units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading raw CW-NIRS data from a SNIRF file and converting it to OD and CONC\n",
    "\n",
    "This notebook uses a finger-tapping dataset in BIDS layout provided by Rob Luke that is automatically fetched. \n",
    "You can also find it [here](https://github.com/rob-luke/BIDS-NIRS-Tapping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get example finger tapping dataset\n",
    "\n",
    "rec = datasets.get_fingertapping()\n",
    "rec[\"od\"] = cedalion.nirs.int2od(rec[\"amp\"])\n",
    "\n",
    "# Plot some data for visual validation\n",
    "f,ax = p.subplots(1,1, figsize=(12,4))\n",
    "ax.plot( rec[\"amp\"].time, rec[\"amp\"].sel(channel=\"S3D3\", wavelength=\"850\"), \"r-\", label=\"850nm\")\n",
    "ax.plot( rec[\"amp\"].time, rec[\"amp\"].sel(channel=\"S3D3\", wavelength=\"760\"), \"r-\", label=\"760nm\")\n",
    "p.legend()\n",
    "ax.set_xlabel(\"time / s\")\n",
    "ax.set_ylabel(\"Signal intensity / a.u.\")\n",
    "\n",
    "display(rec[\"amp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Signal Quality Metrics and applying Masks\n",
    "\n",
    "To assess channel quality metrics such as SNR, channel distances, average amplitudes, sci, and others, we use small helper functions. \n",
    "As input, the quality functions should also expect thresholds for these metrics, so they can feed back both the calculated quality metrics as well as a mask. The masks can then be combined and applied - e.g. to prune channels with low SNR.\n",
    "The input and output arguments are based on xarray time series, quality parameters / instructions for thresholding. \n",
    "The returned mask is a boolean array in the shape and size of the input time series. It indicates where the threshold for our quality metric was passed (\"True\") and is False otherwise.\n",
    "Mmsks can be combined with other masks, for instance to apply several metrics to assess or prune channels. At any point in time, the mask can be applied using the \"apply_mask()\" function available from cedalion's the xrutils package. \n",
    "\n",
    "If you are a user who is mainly interested in high-level application, you can skip to the Section \"***Channel Pruning using Quality Metrics and the Pruning Function***\" below. The \"prune_ch()\" function provides a higher abstraction layer to simply prune your data, using the same metrics and functions that are demonstrated below. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Quality Metrics: SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we assess channel quality by SNR\n",
    "snr_thresh = 16 # the SNR (std/mean) of a channel. Set high here for demonstration purposes\n",
    "\n",
    "# SNR thresholding using the \"snr\" function of the quality subpackage\n",
    "snr, snr_mask = quality.snr(rec[\"amp\"], snr_thresh)\n",
    "\n",
    "# apply mask function. In this example, we want all signals with an SNR below the threshold to be replaced with \"nan\". \n",
    "# We do not want to collapse / combine any dimension of the mask (last argument: \"none\")\n",
    "data_masked_snr_1, masked_elements_1 = xrutils.apply_mask(rec[\"amp\"], snr_mask, \"nan\", \"none\")\n",
    "\n",
    "# alternatively, we can \"drop\" all channels with an SNR below the threshold. Since the SNR of both wavelength might differ\n",
    "# (pass the threshold for one wavelength, but not for the other), we collapse to the \"channel\" dimension.\n",
    "data_masked_snr_2, masked_elements_2 = xrutils.apply_mask(rec[\"amp\"], snr_mask, \"drop\", \"channel\")\n",
    "\n",
    "# show some results\n",
    "print(f\"channels that were masked according to the SNR threshold: {masked_elements_2}\")\n",
    "\n",
    "# dropped:\n",
    "data_masked_snr_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Quality Metrics: Channel Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we assess channel distances. We might want to exclude very short or very long channels\n",
    "sd_threshs = [1, 4.5]*units.cm # defines the lower and upper bounds for the source-detector separation that we would like to keep\n",
    "\n",
    "# Source Detector Separation thresholding\n",
    "ch_dist, sd_mask = quality.sd_dist(rec[\"amp\"], rec.geo3d, sd_threshs)\n",
    "# print the channel distances\n",
    "print(f\"channel distances are: {ch_dist}\")\n",
    "\n",
    "# apply mask function. In this example, we want to \"drop\" all channels that do not fall inside sd_threshs \n",
    "# i.e. drop channels shorter than 1cm and longer than 4.5cm. We want to collapse along the \"channel\" dimension.\n",
    "data_masked_sd, masked_elements = xrutils.apply_mask(rec[\"amp\"], sd_mask, \"drop\", \"channel\")\n",
    "\n",
    "# display the resultings\n",
    "print(f\"channels that were masked according to the SD Distance thresholds: {masked_elements}\")\n",
    "data_masked_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Quality Metrics: Mean Amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we assess average channel amplitudes. We might want to exclude very small or large signals\n",
    "amp_threshs = [0.1, 3]*units.volt # define whether a channel's amplitude is within a certain range\n",
    "\n",
    "# Amplitude thresholding\n",
    "mean_amp, amp_mask = quality.mean_amp(rec[\"amp\"], amp_threshs)\n",
    "\n",
    "# apply mask function. In this example, we want drop all channels that do not fall inside the amplitude thresholds. \n",
    "# We collapse to the \"channel\" dimension.\n",
    "data_masked_amp, masked_elements = xrutils.apply_mask(rec[\"amp\"], amp_mask, \"drop\", \"channel\")\n",
    "\n",
    "# display the results\n",
    "print(f\"channels that were masked according to the amplitude threshold: {masked_elements}\")\n",
    "data_masked_amp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Pruning using Quality Metrics and the Pruning Function\n",
    "\n",
    "To prune channels according to quality criteria, we do not have to manually go through the steps above. Instead, we can create quality masks for the metrics that we are interested in and hand them to a dedicated channel pruning function. The prune function expects a list of quality masks alongside a logical operator that defines how these masks should be combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as above we use three metrics and define thresholds accordingly\n",
    "snr_thresh = 16 # the SNR (std/mean) of a channel. \n",
    "sd_threshs = [1, 4.5]*units.cm # defines the lower and upper bounds for the source-detector separation that we would like to keep\n",
    "amp_threshs = [0.1, 3]*units.volt # define whether a channel's amplitude is within a certain range\n",
    "\n",
    "# then we calculate the masks for each metric: SNR, SD distance and mean amplitude\n",
    "_, snr_mask = quality.snr(rec[\"amp\"], snr_thresh)\n",
    "_, sd_mask = quality.sd_dist(rec[\"amp\"], rec.geo3d, sd_threshs)\n",
    "_, amp_mask = quality.mean_amp(rec[\"amp\"], amp_threshs)\n",
    "\n",
    "# put all masks in a list\n",
    "masks = [snr_mask, sd_mask, amp_mask]\n",
    "\n",
    "# prune channels using the masks and the operator \"all\", which will keep only channels that pass all three metrics\n",
    "amp_pruned, drop_list = quality.prune_ch(rec[\"amp\"], masks, \"all\")\n",
    "\n",
    "# print list of dropped channels\n",
    "print(f\"List of pruned channels: {drop_list}\")\n",
    "# display the new data xarray\n",
    "amp_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Artefact Detection\n",
    "The same xarray-based masks can be used for indicating motion-artefacts. The example below shows how to checks channels for motion artefacts using standard thresholds from Homer2/3. The output is a mask that can be handed to motion correction algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Motion Artifacts and generating the MA mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cedalion.sigproc.artifact as artifact\n",
    "\n",
    "# we use Optical Density data for motion artifact detection\n",
    "fNIRSdata = rec[\"od\"]\n",
    "\n",
    "# define parameters for motion artifact detection. We follow the method from Homer2/3: \"hmrR_MotionArtifactByChannel\" and \"hmrR_MotionArtifact\".\n",
    "t_motion = 0.5*units.s  # time window for motion artifact detection\n",
    "t_mask = 1.0*units.s    # time window for masking motion artifacts (+- t_mask s before/after detected motion artifact)\n",
    "stdev_thresh = 4.0      # threshold for standard deviation of the signal used to detect motion artifacts. Default is 50. We set it very low to find something in our good data for demonstration purposes.\n",
    "amp_thresh = 5.0        # threshold for amplitude of the signal used to detect motion artifacts. Default is 5.\n",
    "\n",
    "# to identify motion artifacts with these parameters we call the following function\n",
    "ma_mask = artifact.id_motion(fNIRSdata, t_motion, t_mask, stdev_thresh, amp_thresh)\n",
    "# it hands us a boolean mask (xarray) of the input dimension, where True a element indicates a motion artifact at a given time point.\n",
    "\n",
    "# show the masks data\n",
    "ma_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output mask is quite detailed and still contains all original dimensions (e.g. single wavelengths) and allows us to combine it with a mask from another motion artifact detection method. This is the same approach as for the channel quality metrics above.\n",
    "While in quality metrics a \"True\" in the mask meant that the sample was above the quality threshold (\"good\"), in motion artifact masks a \"True\" represents an artifact. To invert the logic, simply use the boolean \"not\" operator \"~\": ma_mask_inverted = ~ma_mask.\n",
    "\n",
    "Let us now plot the result for an example channel. Note, that for both wavelengths a different number of artifacts was identified, which can sometimes happen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.figure()\n",
    "p.plot(ma_mask.sel(time=slice(0,250)).time, ma_mask.sel(channel=\"S3D3\", wavelength=\"760\", time=slice(0,250)), \"b-\")\n",
    "p.plot(ma_mask.sel(time=slice(0,250)).time, ma_mask.sel(channel=\"S3D3\", wavelength=\"850\", time=slice(0,250)), \"r-\")\n",
    "p.xlabel(\"time / s\")\n",
    "p.ylabel(\"Motion artifact mask\")\n",
    "p.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our example dataset is very clean. So we artificially detected motion artifacts with a very low threshold. Plotting the mask and the data together (we have to rescale a bit to make both fit): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.figure()\n",
    "p.plot(fNIRSdata.sel(time=slice(0,250)).time, fNIRSdata.sel(channel=\"S3D3\", wavelength=\"760\", time=slice(0,250)), \"r-\")\n",
    "p.plot(ma_mask.sel(time=slice(0,250)).time, ma_mask.sel(channel=\"S3D3\", wavelength=\"850\", time=slice(0,250))/10, \"k-\")\n",
    "p.xlabel(\"time / s\")\n",
    "p.ylabel(\"fNIRS Signal / Motion artifact mask\")\n",
    "p.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refining the MA Mask\n",
    "At the latest when we want to correct motion artifacts, we usually do not need the level of granularity that the mask provides. For instance, we usually want to treat a detected motion artifact in either of both wavelengths or chromophores of one channel as a single artifact that gets flagged for both. We might also want to flag motion artifacts globally, i.e. mask time points for all channels even if only some of them show an artifact. This can easily be done by using the \"id_motion_refine\" function. The function also returns useful information about motion artifacts in each channel in \"ma_info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refine the motion artifact mask. This function collapses the mask along dimensions that are chosen by the \"operator\" argument.\n",
    "# Here we use \"by_channel\", which will yield a mask for each channel by collapsing the masks along either the wavelength or concentration dimension.\n",
    "ma_mask_refined, ma_info = artifact.id_motion_refine(ma_mask, 'by_channel')\n",
    "\n",
    "# show the refined mask\n",
    "ma_mask_refined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the mask does not have the \"wavelength\" or \"concentration\" dimension anymore, and the masks of these dimensions are combined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the figure\n",
    "\n",
    "p.figure()\n",
    "p.plot(fNIRSdata.sel(time=slice(0,250)).time, fNIRSdata.sel(channel=\"S3D3\", wavelength=\"760\", time=slice(0,250)), \"r-\")\n",
    "p.plot(ma_mask_refined.sel(time=slice(0,250)).time, ma_mask_refined.sel(channel=\"S3D3\", time=slice(0,250))/10, \"k-\")\n",
    "p.xlabel(\"time / s\")\n",
    "p.ylabel(\"fNIRS Signal / Refined Motion artifact mask\")\n",
    "p.show() \n",
    "\n",
    "# show the information about the motion artifacts: we get a pandas dataframe telling us \n",
    "# 1) for which channels artifacts were detected, \n",
    "# 2) what is the fraction of time points that were marked as artifacts and\n",
    "# 3) how many artifacts where detected \n",
    "ma_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the \"all\" operator, which will collapse the mask across all dimensions except time, leading to a single motion artifact mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"all\", yields a mask that flags an artifact at any given time if flagged for any channel, wavelength, chromophore, etc.\n",
    "ma_mask_refined, ma_info = artifact.id_motion_refine(ma_mask, 'all')\n",
    "\n",
    "# show the refined mask\n",
    "ma_mask_refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the figure\n",
    "\n",
    "p.figure()\n",
    "p.plot(fNIRSdata.sel(time=slice(0,250)).time, fNIRSdata.sel(channel=\"S3D3\", wavelength=\"760\", time=slice(0,250)), \"r-\")\n",
    "p.plot(ma_mask_refined.sel(time=slice(0,250)).time, ma_mask_refined.sel(time=slice(0,250))/10, \"k-\")\n",
    "p.xlabel(\"time / s\")\n",
    "p.ylabel(\"fNIRS Signal / Refined Motion artifact mask\")\n",
    "p.show() \n",
    "\n",
    "# show the information about the motion artifacts: we get a pandas dataframe telling us \n",
    "# 1) that the mask is for all channels\n",
    "# 2) fraction of time points that were marked as artifacts for this mask across all channels\n",
    "# 3) how many artifacts where detected in total\n",
    "ma_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
