{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cedalion\n",
    "import cedalion.nirs\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pint\n",
    "import matplotlib.pyplot as p\n",
    "import scipy.signal\n",
    "import os.path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Loading SNIRF data\n",
    "\n",
    "This notebook uses a finger-tapping dataset in BIDS layout. Download it [here](https://github.com/rob-luke/BIDS-NIRS-Tapping) and point the variable `DATADIR` to its location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"/home/eike/Projekte/ibslab/30_dev/data/BIDS-NIRS-Tapping\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In its current implementation `read_snirf` extracts three things:\n",
    "- geo is a `xr.DataArray` that stores for coordinates of labeled points (e.g. 'S1', 'D1, 'CZ')\n",
    "- das is a list of `xr.DataArray` that contain the /nirs(i)/data(j)/dataTimeSeries arrays\n",
    "- stim is a `pd.DataFrame` that contains event information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#geo, das, stim = cedalion.io.read_snirf(\"/home/eike/Projekte/ibslab/30_dev/data/BIDS-NIRS-Tapping/sub-01/nirs/sub-01_task-tapping_nirs.snirf\")[0]\n",
    "elements = cedalion.io.read_snirf(os.path.join(DATADIR, \"sub-01/nirs/sub-01_task-tapping_nirs.snirf\"))\n",
    "elem = elements[0] # there is only one NirsElement in this snirf file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Proposal: Use xarray's `DataArray` the main container for data in memory.\n",
    "\n",
    "Note about the term 'channel'. I deviate from the convention that is used in SNIRF and MNE. These toolboxes consider \"S1D1 760nm\" and \"S1D1 850nm\" or \"S1D1 HbO\" and \"S1D1 HbR\" as different channels. I would propose a wording in which \"S1D1\" denotes a channel that comprises multiple components. These components could be for example  different wavelengths or different chromophores. In the context of source separation techniques these components will be futher split up. As will be shown, it is advantageous to keep extra dimensions for these components.\n",
    "\n",
    "\n",
    "recorded time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = elem.data[0] # there is only one data element with amplitude data in this NIRS element. \n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Source, detector and landmark positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "elem.geo3d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Working with named dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.mean(\"time\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Broadcasting works. Numpy functions accept xarrays as arguments. For some of them the output datatype is a xarray again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "od = -np.log(da/da.mean(\"time\"))\n",
    "od"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Selecting data by using coordinates\n",
    "\n",
    "### select by value of coordinate\n",
    "\n",
    "FIXME: support for units in coordinates is work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.sel(wavelength=760.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Select time range\n",
    "\n",
    "We can pass boolean masks to `.sel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.sel(time=(10 < da.time) & (da.time < 20))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### select by regular expression\n",
    "\n",
    "xarrays assign Pandas indexes to coodinates. Functionality to select rows in Pandas DataFrames works here, too. For example we can use Panda's `str` accessor to us string selection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.sel(channel=da.channel.str.match(\"S[2,3]D[1,2]\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Use on xarray to index another\n",
    "\n",
    "The `geo` array stores for labeled points (e.g. 'S1', 'D1', 'CZ') the corresponding 3D coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo3d = elem.geo3d\n",
    "geo3d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "This data array has for the 'channel' dimensions multiple coordinates assigned, the channel labels (e.g. 'S1D1') but also the involved source and detector labels (e.g. 'S1', 'D1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "da"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "We can use the coordinate arrays of `da` to index `geo3d` to get the positions of the sources of all channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo3d.loc[da.source]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "We can calculate the vector from source to detector for each channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo3d.loc[da.detector] - geo3d.loc[da.source]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "... and find the channel distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(geo3d.loc[da.detector] - geo3d.loc[da.source], axis=-1 )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Here three problems arise. First, `np.linalg.norm` returns a normal numpy ndarray. The coordinates are lost. Secondly, the units are lost. And finally, the `axis` parameter of `np.linalg.norm`does not know about the named dimensions and we have to use positional indexing instead.\n",
    "\n",
    "A way to get a xarray return value is to use `xr.apply_ufunc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = geo3d.pint.dequantify() # remove units\n",
    "dists = xr.apply_ufunc(np.linalg.norm, tmp.loc[da.source] - tmp.loc[da.detector], input_core_dims=[[\"pos\"]], kwargs={\"axis\":-1})\n",
    "dists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "There are helper functions in cedalion.xrutils that work around these problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = cedalion.xrutils.norm(geo3d.loc[da.source] - geo3d.loc[da.detector], dim=\"pos\")\n",
    "dists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "# Stim\n",
    "\n",
    "use pandas' DataFrame to store tabular stimulus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim = elem.stim\n",
    "# rename trial_types\n",
    "stim.loc[stim.trial_type == \"1.0\", \"trial_type\"] = \"control\"\n",
    "stim.loc[stim.trial_type == \"2.0\", \"trial_type\"] = \"Tapping/Left\"\n",
    "stim.loc[stim.trial_type == \"3.0\", \"trial_type\"] = \"Tapping/Right\"\n",
    "stim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Using the onset times in the `stim` DataFrame we can split the time series into epochs. This functionality could in principle be applied to any xarray that has a 'time' dimension. So it might make sense to provide this functionality as an accesor. For the moment, the `cd` (short for cedalion) has been defined an can be accessed like that. \n",
    "\n",
    "Note that in the epoched DataArray the 'time' dimensions has been renamed to 'reltime'. Also the trial_types have been added as coordinates for the 'epoch' dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = da.cd.to_epochs(stim, [\"Tapping/Left\", \"Tapping/Right\"], before=10, after=30)\n",
    "epochs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "# Beer-Lambert\n",
    "\n",
    "[Scott Prahl's tabulated extinction coefficients](https://omlc.org/spectra/hemoglobin/index.html) are implemented in cedalion. Use the wavelength coordinates to query them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = cedalion.nirs.get_extinction_coefficients(\"prahl\", da.wavelength)\n",
    "E"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Calculate the inverse matrix. /!\\ Here the order of the dimensions matter. We need to tell xr.apply_ufunc to reverse the order of dimensions. Also xr.apply_ufunc struggles with quantified DataArrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = E.pint.dequantify()\n",
    "Einv = xr.apply_ufunc(\n",
    "    np.linalg.pinv, \n",
    "    tmp, \n",
    "    input_core_dims=[[\"chromo\", \"wavelength\"]], \n",
    "    output_core_dims=[[\"wavelength\", \"chromo\"]])\n",
    "Einv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Check that Einv is the inverse of E by using numpy matrix multiplication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "(E.values @ Einv.values).round(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "Again, cedalion.xrutils has a wrapper for np.linalg.pinv that takes care of these problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "Einv = cedalion.xrutils.pinv(E)\n",
    "display(Einv)\n",
    "(E.values @ Einv.values).round(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Define an array of differential pathlengths factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpf = xr.DataArray([6, 6], dims=\"wavelength\", coords={\"wavelength\" : [760., 850.]})\n",
    "dpf = dpf.pint.quantify(\"1\") # differential path lengths factors are unitless\n",
    "dpf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Calculate optical densities, divide by channel distances and dpfs. Then use matrix multiplciation to apply `Einv`. The matrix multiplication sums over the wavelength dimension. Note how the wavelength dimension automatically is replaced by the 'chromo' dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "optical_density = - np.log( da / da.mean(\"time\") )\n",
    "conc = Einv @ (optical_density / ( dists * dpf))\n",
    "conc = conc.pint.to(\"micromolar\")\n",
    "conc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Footgun: Note that xarrays `@` operator behaves differently. Here the left and right array match in two dimension and `@` sums each up, contracting the matrix to a single scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "E @ Einv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Plot the concentration time traces. Use `.sel` to select channels and chromophore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.plot(conc.time, conc.sel(channel=\"S5D7\", chromo=\"HbO\"), \"r-\")\n",
    "p.plot(conc.time, conc.sel(channel=\"S5D7\", chromo=\"HbR\"), \"b-\")\n",
    "p.xlim(500,700)\n",
    "for i, r in stim.iterrows():\n",
    "    p.axvline(r.onset)\n",
    "    p.axvline(r.onset+r.duration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "# Frequency Filter\n",
    "\n",
    "Construct a 4th order Butterworth bandpass filter with $f_{min}=0.02\\, \\textrm{Hz}$ and $f_{max}=0.5\\, \\textrm{Hz}$. Use again `xr.apply_ufunc` to apply `scipy.signal.filtfilt` and get a xarray return value.\n",
    "\n",
    "Again xr.apply_ufunc strips the units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fny = da.cd.sampling_rate/2\n",
    "b,a = scipy.signal.butter(4, (0.02/fny, 0.5/fny), \"bandpass\")\n",
    "conc_filtered = xr.apply_ufunc(scipy.signal.filtfilt, b,a, conc)\n",
    "conc_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_filtered = conc.cd.freq_filter(0.02, 0.5, 4)\n",
    "conc_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.plot(conc_filtered.time, conc_filtered.sel(channel=\"S5D7\", chromo=\"HbO\"), \"r-\")\n",
    "p.plot(conc_filtered.time, conc_filtered.sel(channel=\"S5D7\", chromo=\"HbR\"), \"b-\")\n",
    "p.xlim(500,700)\n",
    "for i, r in stim.iterrows():\n",
    "    p.axvline(r.onset)\n",
    "    p.axvline(r.onset+r.duration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "# Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_epochs = conc_filtered.cd.to_epochs(stim, [\"Tapping/Left\", \"Tapping/Right\"], before=5, after=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_epochs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "# Block averages\n",
    "\n",
    "To calculate average responses we need to calculate the baseline before each stimulus and subtract it. The time samples beloning to the baseline are easily selected by `conc_epochs.reltime < 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = conc_epochs.sel(reltime=(conc_epochs.reltime < 0)).mean(\"reltime\")\n",
    "conc_epochs_blcorrected = conc_epochs - baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "Now all epochs belonging to a trial_type need to be grouped and averaged. Xarray's `groupby` operation makes that easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "blockaverage = conc_epochs_blcorrected.groupby(\"trial_type\").mean(\"epoch\")\n",
    "blockaverage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = p.subplots(5,6, figsize=(12,10))\n",
    "ax = ax.flatten()\n",
    "for i_ch, ch in enumerate(conc_epochs_blcorrected.channel):\n",
    "    for ls, trial_type in zip([\"-\", \"--\"], blockaverage.trial_type):\n",
    "        #for i_epoch in range(epochs.shape[0]):\n",
    "        #    ax[i_ch].plot(conc_epochs_blcorrected.reltime, conc_epochs_blcorrected.loc[i_epoch, \"HbO\", ch, :], \"r-\", alpha=.1)\n",
    "        #    ax[i_ch].plot(conc_epochs_blcorrected.reltime, conc_epochs_blcorrected.loc[i_epoch, \"HbR\", ch, :], \"b-\", alpha=.1)\n",
    "    \n",
    "        ax[i_ch].plot(blockaverage.reltime, blockaverage.sel(chromo=\"HbO\", trial_type=trial_type, channel=ch), \"r\", lw=2, ls=ls)\n",
    "        ax[i_ch].plot(blockaverage.reltime, blockaverage.sel(chromo=\"HbR\", trial_type=trial_type, channel=ch), \"b\", lw=2, ls=ls)\n",
    "        ax[i_ch].grid(1)\n",
    "        ax[i_ch].set_title(ch.values)\n",
    "        ax[i_ch].set_ylim(-.2, .25)\n",
    "    \n",
    "p.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "# sklearn\n",
    "Evaluate the interplay of xarray and sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "Start from baseline-corrected, epoched concentration data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_epochs_blcorrected"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "We don't need the baseline samples so remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_epochs_blcorrected_nobl = conc_epochs_blcorrected.sel(reltime=conc_epochs_blcorrected.reltime >=0)\n",
    "conc_epochs_blcorrected_nobl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_epochs_blcorrected_nobl.channel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "sklearn estimator and transforms expect datasets in the form of an 2D array X with shape (n_samples, n_features). We can transform our 4D DataArray into this shape by stacking 3 dimensions together. Also create an index for the epoch dimension, which did not have one so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = conc_epochs_blcorrected_nobl.stack(features=[\"chromo\", \"channel\", \"reltime\"])\n",
    "X = X.set_xindex(\"trial_type\")\n",
    "X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "For a classification taks we need an array with class labels, typically called `y`. We can use sklearn's LabelEncoder to derive labels from the trial_type coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = xr.apply_ufunc(LabelEncoder().fit_transform, X.trial_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(y)\n",
    "display(y.sel(trial_type=\"Tapping/Left\"))\n",
    "display(y.sel(trial_type=\"Tapping/Right\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "Sklearn's train_test_split works on xarrays and does return xarrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train.groupby(\"trial_type\").count().rename(\"train\"))\n",
    "display(X_test.groupby(\"trial_type\").count().rename(\"test\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "Train a LDA classifier and use it to predict labels. We need to use `xr.apply_ufunc` if we want to have an xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearDiscriminantAnalysis(n_components=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xr.apply_ufunc(clf.predict, X_test, input_core_dims=[[\"features\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "The advantage here is that when X and y are still xarrays we still have access to the coordinate axes. That means for example, that we can still use them to select samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.figure()\n",
    "bincount, bins, _ = p.hist(clf.decision_function(X_train.sel(trial_type=\"Tapping/Left\")), alpha=.5, fc=\"r\")\n",
    "bincount, bins, _ = p.hist(clf.decision_function(X_train.sel(trial_type=\"Tapping/Right\")), bins, alpha=.5, fc=\"g\")\n",
    "\n",
    "p.figure()\n",
    "bincount, bins, _ = p.hist(clf.decision_function(X_test.sel(trial_type=\"Tapping/Left\")), alpha=.5, fc=\"r\")\n",
    "bincount, bins, _ = p.hist(clf.decision_function(X_test.sel(trial_type=\"Tapping/Right\")), bins, alpha=.5, fc=\"g\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "Finally, test to use cross-validation while training our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(clf, X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
