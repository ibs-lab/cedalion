{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Image Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cells setups the environment when executed in Google Colab.\n",
    "try:\n",
    "    import google.colab\n",
    "    !curl -s https://raw.githubusercontent.com/ibs-lab/cedalion/dev/scripts/colab_setup.py -o colab_setup.py\n",
    "    # Select branch with --branch \"branch name\" (default is \"dev\")\n",
    "    %run colab_setup.py\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Notebook configuration \n",
    "Decide for an example dataset with a sparse probe or a high-density probe for DOT. \n",
    "The notebook will load example data accordingly.\n",
    "\n",
    "Also specify, if precomputed results of the photon propagation should be used and\n",
    "if the 3D visualizations should be interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose between two datasets\n",
    "DATASET = \"fingertappingDOT\" # high-density montage\n",
    "#DATASET = \"fingertapping\"   # sparse montage\n",
    "\n",
    "# choose a head model\n",
    "HEAD_MODEL = \"colin27\"\n",
    "# HEAD_MODEL = \"icbm152\"\n",
    "\n",
    "# choose between the monte\n",
    "FORWARD_MODEL = \"MCX\" # photon monte carlo\n",
    "#FORWARD_MODEL = \"NIRFASTER\" # finite element method - NOTE, you must have NIRFASTer installed via runnning <$ bash install_nirfaster.sh CPU # or GPU> from a within your cedalion root directory.\n",
    "\n",
    "# set this flag to False to actual compute the forward model results\n",
    "PRECOMPUTED_FLUENCE = True\n",
    "\n",
    "# set this flag to True to enable interactive 3D plots\n",
    "INTERACTIVE_PLOTS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "\n",
    "if INTERACTIVE_PLOTS:\n",
    "    pv.set_jupyter_backend('server')\n",
    "else:\n",
    "    pv.set_jupyter_backend('static')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import matplotlib.pyplot as p\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from IPython.display import Image\n",
    "from pint.errors import UnitStrippedWarning\n",
    "\n",
    "import cedalion\n",
    "import cedalion.dataclasses as cdc\n",
    "import cedalion.datasets\n",
    "import cedalion.geometry.registration\n",
    "import cedalion.geometry.segmentation\n",
    "import cedalion.imagereco.forward_model as fw\n",
    "import cedalion.imagereco.tissue_properties\n",
    "import cedalion.io\n",
    "import cedalion.plots\n",
    "import cedalion.sigproc.quality as quality\n",
    "import cedalion.sigproc.motion_correct as motion_correct\n",
    "import cedalion.vis.plot_sensitivity_matrix\n",
    "from cedalion import units\n",
    "from cedalion.imagereco.solver import pseudo_inverse_stacked\n",
    "from cedalion.io.forward_model import FluenceFile, load_Adot\n",
    "\n",
    "import cedalion.xrutils as xrutils\n",
    "xrutils.unit_stripping_is_error()\n",
    "\n",
    "\n",
    "xr.set_options(display_expand_data=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to display gifs in rendered notbooks\n",
    "def display_image(fname : str):\n",
    "    display(Image(data=open(fname,'rb').read(), format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Working Directory\n",
    "In this notebook the output of the fluence and sensitivity calculations are stored in a temporary directory. This will be \n",
    "deleted when the notebook ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporary_directory = TemporaryDirectory()\n",
    "tmp_dir_path = Path(temporary_directory.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Load a finger-tapping dataset \n",
    "\n",
    "For this demo we load an example finger-tapping recording through either `cedalion.datasets.get_fingertapping` or `cedalion.datasets.get_fingertappingDOT`. \n",
    "\n",
    "The snirf files of these datasets contains a single NIRS element with one block of raw amplitude data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"fingertappingDOT\":\n",
    "    rec = cedalion.datasets.get_fingertappingDOT()\n",
    "elif DATASET == \"fingertapping\":\n",
    "    rec = cedalion.datasets.get_fingertapping()\n",
    "else:\n",
    "    raise ValueError(\"unknown dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "The location of the probes is obtained from the snirf metadata (i.e. /nirs0/probe/)\n",
    "\n",
    "Note that units ('m') are adopted and the coordinate system is named 'digitized'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo3d_meas = rec.geo3d\n",
    "display(geo3d_meas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the montage\n",
    "cedalion.plots.plot_montage3D(rec[\"amp\"], geo3d_meas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The measurement list is a `pandas.DataFrame` that describes which source-detector pairs form channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_list = rec._measurement_lists[\"amp\"]\n",
    "display(meas_list.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Event/stimulus information is also stored in a `pandas.DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.stim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "For clarity, events are given more descriptive names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if DATASET == \"fingertappingDOT\":\n",
    "   rec.stim.cd.rename_events( {\n",
    "        \"1\": \"Control\", \n",
    "        \"2\": \"FTapping/Left\", \n",
    "        \"3\": \"FTapping/Right\",\n",
    "        \"4\": \"BallSqueezing/Left\",\n",
    "        \"5\": \"BallSqueezing/Right\"\n",
    "    } )\n",
    "elif DATASET == \"fingertapping\":\n",
    "    rec.stim.cd.rename_events( {\n",
    "        \"1.0\": \"Control\", \n",
    "        \"2.0\": \"FTapping/Left\", \n",
    "        \"3.0\": \"FTapping/Right\"\n",
    "    } )\n",
    "\n",
    "# count number of trials per trial_type\n",
    "display(\n",
    "    rec.stim.groupby(\"trial_type\")[[\"onset\"]]\n",
    "    .count()\n",
    "    .rename({\"onset\": \"#trials\"}, axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Perform motion correction, conversion to optical density and bandpass filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec[\"od\"] = cedalion.nirs.int2od(rec[\"amp\"])\n",
    "rec[\"od_tddr\"] = motion_correct.tddr(rec[\"od\"])\n",
    "rec[\"od_wavelet\"] = motion_correct.wavelet(rec[\"od_tddr\"])\n",
    "\n",
    "# bandpass filter the data\n",
    "rec[\"od_freqfiltered\"] = rec[\"od_wavelet\"].cd.freq_filter(\n",
    "    fmin=0.01, fmax=0.5, butter_order=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Calculate block averages in optical density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment data into epochs\n",
    "epochs = rec[\"od_freqfiltered\"].cd.to_epochs(\n",
    "    rec.stim,  # stimulus dataframe\n",
    "    [\"FTapping/Left\", \"FTapping/Right\"],  # select fingertapping events, discard others\n",
    "    before=5 * units.s,  # seconds before stimulus\n",
    "    after=30 * units.s,  # seconds after stimulus\n",
    ")\n",
    "\n",
    "# calculate baseline\n",
    "baseline = epochs.sel(reltime=(epochs.reltime < 0)).mean(\"reltime\")\n",
    "\n",
    "# subtract baseline\n",
    "epochs_blcorrected = epochs - baseline\n",
    "\n",
    "# group trials by trial_type. For each group individually average the epoch dimension\n",
    "blockaverage = epochs_blcorrected.groupby(\"trial_type\").mean(\"epoch\")\n",
    "\n",
    "# Plot block averages. Please ignore errors if the plot is too small in the HD case\n",
    "\n",
    "noPlts2 = int(np.ceil(np.sqrt(len(blockaverage.channel))))\n",
    "f,ax = p.subplots(noPlts2,noPlts2, figsize=(12,10))\n",
    "ax = ax.flatten()\n",
    "for i_ch, ch in enumerate(blockaverage.channel):\n",
    "    for ls, trial_type in zip([\"-\", \"--\"], blockaverage.trial_type):\n",
    "        ax[i_ch].plot(blockaverage.reltime, blockaverage.sel(wavelength=760, trial_type=trial_type, channel=ch), \"r\", lw=2, ls=ls)\n",
    "        ax[i_ch].plot(blockaverage.reltime, blockaverage.sel(wavelength=850, trial_type=trial_type, channel=ch), \"b\", lw=2, ls=ls)\n",
    "\n",
    "    ax[i_ch].grid(1)\n",
    "    ax[i_ch].set_title(ch.values)\n",
    "    ax[i_ch].set_ylim(-.02, .02)\n",
    "    ax[i_ch].set_axis_off()\n",
    "    ax[i_ch].axhline(0, c=\"k\")\n",
    "    ax[i_ch].axvline(0, c=\"k\")\n",
    "\n",
    "p.suptitle(\"760nm: r | 850nm: b | left: - | right: --\")\n",
    "p.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Load segmented MRI scan\n",
    "\n",
    "For this example use a segmentation of the Colin27 average brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HEAD_MODEL == \"colin27\":\n",
    "    SEG_DATADIR, mask_files, landmarks_file = cedalion.datasets.get_colin27_segmentation()\n",
    "    PARCEL_DIR = cedalion.datasets.get_colin27_parcel_file()\n",
    "elif HEAD_MODEL == \"icbm152\":\n",
    "    SEG_DATADIR, mask_files, landmarks_file = cedalion.datasets.get_icbm152_segmentation()\n",
    "    PARCEL_DIR = cedalion.datasets.get_icbm152_parcel_file()\n",
    "else:\n",
    "    raise ValueError(\"unknown head model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "The segmentation masks are in individual niftii files. The dict `mask_files` maps mask filenames relative to `SEG_DATADIR` to short labels. These labels describe the tissue type of the mask. \n",
    "\n",
    "In principle the user is free to choose these labels. However, they are later used to lookup the tissue's optical properties. So they must be map to one of the tabulated tissue types (c.f. `cedalion.imagereco.tissue_properties.TISSUE_LABELS`).\n",
    "\n",
    "The variable `landmarks_file` holds the path to a file containing landmark positions in scanner space (RAS). This file can be created with Slicer3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(SEG_DATADIR)\n",
    "display(mask_files)\n",
    "display(landmarks_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Coordinate systems\n",
    "\n",
    "Up to now we have geometrical data from three different coordinate reference systems (CRS):\n",
    "\n",
    "- The optode positions are in one space `CRS='digitized'` and the coordinates are in meter. In our example the origin is at the head center and y-axis pointing in the superior direction. Other digitization tools can use other units or coordinate systems.\n",
    "- The segmentation masks are in voxel space (`CRS='ijk'`) in which the voxel edges are aligned with the coordinate axes. Each voxel has unit edge length, i.e. coordinates are dimensionless. \n",
    "  Axis-aligned grids are computationally efficient, which is why the photon simulation code (MCX) uses this coordinate system.\n",
    "- The voxel space (`CRS='ijk'`) is related to scanner space (`CRS='ras'` or `CRS='aligned'`) in which coordinates have physical units and coordinate axes point to the (r)ight, (a)nterior and s(uperior) directions. The relation between both spaces is given through an affine transformation (e.g. `t_ijk2ras`). When loading the segmentation masks in Slicer3D this transformation is automatically applied. Hence, the picked landmark coordinates are exported in RAS space.\n",
    "\n",
    "  The niftii file provides a string label for the scanner space. In this example the RAS space is called 'aligned' because the masks are aligned to another MRI scan.\n",
    "\n",
    "\n",
    "To avoid confusion between these different coordinate systems, `cedalion` tries to be explicit about which CRS a given point cloud or surface is in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## The TwoSurfaceHeadModel\n",
    "\n",
    "The photon propagation considers the complete MRI scan, in which each voxel is attributed to one tissue type with its respective optical properties. However, the image reconstruction does not intend to reconstruct absorption changes in each voxel. The inverse problem is simplified, by considering only two surfaces (scalp and brain) and reconstruct only absorption changes in voxels close to these surfaces.\n",
    "\n",
    "The class `cedalion.imagereco.forward_model.TwoSurfaceHeadModel` groups together the segmentation mask, landmark positions and affine transformations as well as the scalp and brain surfaces. The brain surface is calculated by grouping together white and gray matter masks. The scalp surface encloses the whole head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = fw.TwoSurfaceHeadModel.from_surfaces(\n",
    "    segmentation_dir=SEG_DATADIR,\n",
    "    mask_files = mask_files,\n",
    "    brain_surface_file= os.path.join(SEG_DATADIR, \"mask_brain.obj\"),\n",
    "    scalp_surface_file= os.path.join(SEG_DATADIR, \"mask_scalp.obj\"),\n",
    "    landmarks_ras_file=landmarks_file,\n",
    "    parcel_file=PARCEL_DIR,\n",
    "    brain_face_count=None,\n",
    "    scalp_face_count=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "head.segmentation_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "head.landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "head.brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "head.scalp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "`TwoSurfaceHeadModel.from_surfaces` converts everything into voxel space (`CRS='ijk'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "head.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "The transformation matrix to translate from voxel to scanner space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "head.t_ijk2ras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Changing between coordinate systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_ras = head.apply_transform(head.t_ijk2ras)\n",
    "display(head_ras.crs)\n",
    "display(head_ras.brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Optode Registration\n",
    "The optode coordinates from the recording must be aligned with the scalp surface. Currently, `cedaÄºion` offers a simple registration method, which finds an affine transformation (scaling, rotating, translating) that matches the landmark positions of the head model and their digitized counter parts. Afterwards, optodes are snapped to the nearest vertex on the scalp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo3d_snapped_ijk = head.align_and_snap_to_scalp(geo3d_meas)\n",
    "display(geo3d_snapped_ijk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = pv.Plotter()\n",
    "cedalion.plots.plot_surface(plt, head.brain, color=\"w\")\n",
    "cedalion.plots.plot_surface(plt, head.scalp, opacity=.1)\n",
    "cedalion.plots.plot_labeled_points(plt, geo3d_snapped_ijk)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Simulate light propagation in tissue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "`cedalion.imagereco.forward_model.ForwardModel` is a wrapper around pmcx. Using the data in the head model it prepares the inputs for either pmcx or NIRFASTer and offers functionality to calculate the sensitivty matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwm = cedalion.imagereco.forward_model.ForwardModel(head, geo3d_snapped_ijk, meas_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Run the simulation\n",
    "\n",
    "The `compute_fluence_mcx` and `compute_fluence_nirfaster` methods simulate a light source at each optode position and calculate the fluence in each voxel. By setting `RUN_PACKAGE`, you can choose between the pmcx or NIRFASTer package to perform this simulation.\n",
    "PLEASE NOTE: if you USE_CACHED data (download the example data) be aware that the file is quite big (~2GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRECOMPUTED_FLUENCE:\n",
    "    if FORWARD_MODEL == \"MCX\":\n",
    "        fluence_fname = cedalion.datasets.get_precomputed_fluence(DATASET, HEAD_MODEL)\n",
    "    elif FORWARD_MODEL == \"NIRFASTER\":\n",
    "        raise NotImplementedError(\n",
    "            \"Currently there are no precomputed NIRFASTER results available\"\n",
    "        )\n",
    "else:\n",
    "    fluence_fname = tmp_dir_path / \"fluence.h5\"\n",
    "\n",
    "    if FORWARD_MODEL == \"MCX\":\n",
    "        fwm.compute_fluence_mcx(fluence_fname)\n",
    "    elif FORWARD_MODEL == \"NIRFASTER\":\n",
    "        fwm.compute_fluence_nirfaster(fluence_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "The photon simulation yields the fluence in each voxel for each wavelength:\n",
    "\n",
    "- `fluence_all` is a `xr.DataArray` with dimensions: ('label', 'wavelength', 'i', 'j', 'k'),\n",
    "\n",
    "   i.e. for each optode and wavelength it stores the 3D image of the computed fluence in each voxel\n",
    "\n",
    "- `fluence_at_optodes` is a `xr.DataArray` with dimensions: ('optode1', 'optode2', 'wavelength').\n",
    "\n",
    "  It contains the fluence directly at the position of the optodes, used for normalization purposes.\n",
    "\n",
    "\n",
    "Both arrays are stored on disk in the hdf5 file at `fluence_fname` and should be queried through `cedalion.io.forward_model.FluenceFile`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Also, for a each combination of two optodes, the fluence in the voxels at the optode positions is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## Plot fluence\n",
    "\n",
    "To illustrate the tissue probed by light travelling from a source to the detector two fluence profiles need to be multiplied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting use a geo3d without the landmarks\n",
    "geo3d_plot = geo3d_snapped_ijk[geo3d_snapped_ijk.type != cdc.PointType.LANDMARK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "tags": [
     "nbsphinx-thumbnail"
    ]
   },
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "\n",
    "plt = pv.Plotter()\n",
    "\n",
    "if DATASET == \"fingertappingDOT\":\n",
    "    src, det, wl = \"S5\", \"D16\", 760\n",
    "elif DATASET == \"fingertapping\":\n",
    "    src, det, wl = \"S2\", \"D3\", 760\n",
    "else:\n",
    "    raise ValueError(\"unknown dataset\")\n",
    "\n",
    "# fluence_file.get_fluence returns a 3D numpy array with the fluence\n",
    "# for a specified source and wavelength.\n",
    "with FluenceFile(fluence_fname) as fluence_file:\n",
    "    f = fluence_file.get_fluence(src, wl) * fluence_file.get_fluence(det, wl)\n",
    "\n",
    "f[f <= 0] = f[f > 0].min()\n",
    "f = np.log10(f)\n",
    "vf = pv.wrap(f)\n",
    "\n",
    "plt.add_volume(\n",
    "    vf,\n",
    "    log_scale=False,\n",
    "    cmap=\"plasma_r\",\n",
    "    clim=(-10, 0),\n",
    ")\n",
    "cedalion.plots.plot_surface(plt, head.brain, color=\"w\")\n",
    "cedalion.plots.plot_labeled_points(plt, geo3d_plot, show_labels=False)\n",
    "\n",
    "cog = head.brain.vertices.mean(\"label\")\n",
    "cog = cog.pint.dequantify().values\n",
    "plt.camera.position = cog + [-300, 30, 100]\n",
    "plt.camera.focal_point = cog\n",
    "plt.camera.up = [0, 0, 1]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### Calculate the sensitivity matrices\n",
    "The forward model's function `compute_sensitivity` calculates the sensitivity matrix from the fluence file and saves the result in a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_fname = tmp_dir_path / \"sensitivity.h5\"\n",
    "fwm.compute_sensitivity(fluence_fname, sensitivity_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "The sensitivity matrix describes how an absorption change at a given surface vertex changes the optical density in a given channel and wavelength. \n",
    "\n",
    "The coordinate `is_brain` holds a mask to distinguish brain and scalp voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and display sensitivity matrix\n",
    "Adot = load_Adot(sensitivity_fname)\n",
    "display(Adot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### Plot Sensitivity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = cedalion.vis.plot_sensitivity_matrix.Main(\n",
    "    sensitivity=Adot,\n",
    "    brain_surface=head.brain,\n",
    "    head_surface=head.scalp,\n",
    "    labeled_points=geo3d_plot,\n",
    ")\n",
    "plotter.plot(high_th=0, low_th=-3)\n",
    "plotter.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "The sensitivity `Adot` has shape (nchannel, nvertex, nwavelenghts). To solve the inverse problem we need a matrix that relates OD in channel space to concentration changes in image space. Hence, the sensitivity must include the extinction coefficients to translate between OD and concentrations. Furthermore, channels at different wavelengths and vertices at different chromophores  must be  stacked into new dimensions (called `'flat_channel'` and `'flat_vertex'`) to yield the following matrix equation:\n",
    "\n",
    "$$ \\left( \\begin{matrix} OD_{c_1, \\lambda_1} \\\\ \\vdots \\\\ OD_{c_N,\\lambda_1} \\\\ OD_{c_1,\\lambda_2} \\\\ \\vdots \\\\ OD_{c_N,\\lambda_2} \\end{matrix}\\right) = A \\cdot\n",
    "\\left( \\begin{matrix} \\Delta c_{v_1, HbO} \\\\ \\vdots \\\\ \\Delta c_{v_N, HbO} \\\\ \\Delta c_{v_1, HbR} \\\\ \\vdots \\\\ \\Delta c_{v_N, HbR} \\end{matrix}\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adot_stacked = fwm.compute_stacked_sensitivity(Adot)\n",
    "Adot_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### Invert the sensitivity matrix\n",
    "\n",
    "With `'Adot_stacked'` now being a 2D matrix, its pseudo-inverse can be computed with the function `pseude_inverse_stacked`. Different regularization options are implemented and can be controled through the parameters `alpha`, `alpha_spatial` and `Cmeas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = pseudo_inverse_stacked(Adot_stacked, alpha = 0.01, alpha_spatial = 0.001)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "### Calculate concentration changes\n",
    "\n",
    "- the optical density has shape (nchannel, nwavelength, time). Additional dimensions like 'trial_type' in this example are allowed, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "blockaverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "To apply the inverted sensitiviy matrix, the OD wavelength and channel dimensions need to be stacked. Then \n",
    "the inverted sensitivity matrix can be multiplied which contracts over `'flat_channel'` and the `'flat_vertex'` dimension remains.\n",
    "The `'flat_vertex'` dimensions contains vertices of the scalp and the brain for both chromophores. These need to be\n",
    "separated. The function `fw.apply_inv_sensitiviy` takes care of all of this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "dC_brain, dC_scalp = fw.apply_inv_sensitivity(blockaverage, B)\n",
    "display(dC_brain)\n",
    "display(dC_scalp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "Convert concentration changes into micromolar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dC_brain = dC_brain.pint.quantify().pint.to(\"uM\").pint.dequantify()\n",
    "dC_scalp = dC_scalp.pint.quantify().pint.to(\"uM\").pint.dequantify()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "## Visualizing image reconstruction results\n",
    "\n",
    "In the following, different cedalion plot functions will be showcased to visualize concentration changes on the brain and scalp surfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### Channel Space\n",
    "\n",
    "The function `cedalion.plots.scalp_plot_gif` allows to create an animated gif of channel-space OD changes projected on the scalp.\n",
    "\n",
    "Here, it is used to show the time course of the blockaveraged OD changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cedalion.plots import scalp_plot_gif\n",
    "\n",
    "# configure the plot\n",
    "data_ts = blockaverage.sel(wavelength=850, trial_type=\"FTapping/Right\")\n",
    "data_ts = data_ts.rename({\"reltime\": \"time\"})\n",
    "geo3d = rec.geo3d\n",
    "filename_scalp = \"scalp_plot_ts\"\n",
    "\n",
    "# call plot function\n",
    "scalp_plot_gif(\n",
    "    data_ts,\n",
    "    geo3d,\n",
    "    filename=filename_scalp,\n",
    "    time_range=(-5, 30, 0.5) * units.s,\n",
    "    scl=(-0.01, 0.01),\n",
    "    fps=6,\n",
    "    optode_size=6,\n",
    "    optode_labels=True,\n",
    "    str_title=\"OD 850 nm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(f\"{filename_scalp}.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "### Image Space\n",
    "\n",
    "#### Single-View Animations of Activitations on the Brain\n",
    "\n",
    "The function `cedalion.plots.image_recon_view` allows to create an animated gif of image-space concentration changes projected on the brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cedalion.plots import image_recon_view\n",
    "\n",
    "filename_view = 'image_recon_view'\n",
    "\n",
    "X_ts = xr.concat([dC_brain.sel(trial_type=\"FTapping/Right\"), dC_scalp.sel(trial_type=\"FTapping/Right\")], dim=\"vertex\")\n",
    "X_ts = X_ts.rename({\"reltime\": \"time\"})\n",
    "X_ts = X_ts.transpose(\"vertex\", \"chromo\", \"time\")\n",
    "X_ts = X_ts.assign_coords(is_brain=('vertex', Adot.is_brain.values))\n",
    "\n",
    "scl = np.percentile(np.abs(X_ts.sel(chromo='HbO').values.reshape(-1)),99)\n",
    "clim = (-scl,scl)\n",
    "\n",
    "image_recon_view(\n",
    "    X_ts,  # time series data; can be 2D (static) or 3D (dynamic)\n",
    "    head,\n",
    "    cmap='seismic',\n",
    "    clim=clim,\n",
    "    view_type='hbo_brain',\n",
    "    view_position='left',\n",
    "    title_str='HbO / uM',\n",
    "    filename=filename_view,\n",
    "    SAVE=True,\n",
    "    time_range=(-5,30,0.5)*units.s,\n",
    "    fps=6,\n",
    "    geo3d_plot = geo3d_plot,\n",
    "    wdw_size = (1024, 768)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(f\"{filename_view}.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "Alternatively, we can just select a single time point and plot activity as a still image at that time. Note the different file suffix (.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selects the nearest time sample at t=4s in X_ts\n",
    "X_ts_plot = X_ts.sel(time=4, method=\"nearest\") # note: sel does not accept quantified units\n",
    "\n",
    "filename_view = 'image_recon_view_still'\n",
    "\n",
    "image_recon_view(\n",
    "    X_ts_plot,  # time series data; can be 2D (static) or 3D (dynamic)\n",
    "    head,\n",
    "    cmap='seismic',\n",
    "    clim=clim,\n",
    "    view_type='hbo_brain',\n",
    "    view_position='left',\n",
    "    title_str='HbO / uM',\n",
    "    filename=filename_view,\n",
    "    SAVE=True,\n",
    "    time_range=(-5,30,0.5)*units.s,\n",
    "    fps=6,\n",
    "    geo3d_plot = geo3d_plot,\n",
    "    wdw_size = (1024, 768)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(f\"{filename_view}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "#### Multi-View Animations of Activitations on the Brain\n",
    "\n",
    "The function `cedalion.plots.image_recon_multi_view` shows the activity on the brain from all angles as still image or animated across time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cedalion.plots import image_recon_multi_view\n",
    "\n",
    "filename_multiview = 'image_recon_multiview'\n",
    "\n",
    "# prepare data\n",
    "X_ts = xr.concat([dC_brain.sel(trial_type=\"FTapping/Right\"), dC_scalp.sel(trial_type=\"FTapping/Right\")], dim=\"vertex\")\n",
    "X_ts = X_ts.rename({\"reltime\": \"time\"})\n",
    "X_ts = X_ts.transpose(\"vertex\", \"chromo\", \"time\")\n",
    "X_ts = X_ts.assign_coords(is_brain=('vertex', Adot.is_brain.values))\n",
    "\n",
    "scl = np.percentile(np.abs(X_ts.sel(chromo='HbO').values.reshape(-1)),99)\n",
    "clim = (-scl,scl)\n",
    "\n",
    "\n",
    "image_recon_multi_view(\n",
    "    X_ts,  # time series data; can be 2D (static) or 3D (dynamic)\n",
    "    head,\n",
    "    cmap='seismic',\n",
    "    clim=clim,\n",
    "    view_type='hbo_brain',\n",
    "    title_str='HbO / uM',\n",
    "    filename=filename_multiview,\n",
    "    SAVE=True,\n",
    "    time_range=(-5,30,0.5)*units.s,\n",
    "    fps=6,\n",
    "    geo3d_plot = None, #  geo3d_plot\n",
    "    wdw_size = (1024, 768)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(f\"{filename_multiview}.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "#### Multi-View Animations of Activitations on the Scalp\n",
    "This gives us activity on the scalp after recon from all angles as still image or across time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cedalion.plots import image_recon_multi_view\n",
    "\n",
    "filename_multiview_scalp = 'image_recon_multiview_scalp'\n",
    "\n",
    "# prepare data\n",
    "X_ts = xr.concat([dC_brain.sel(trial_type=\"FTapping/Right\"), dC_scalp.sel(trial_type=\"FTapping/Right\")], dim=\"vertex\")\n",
    "X_ts = X_ts.rename({\"reltime\": \"time\"})\n",
    "X_ts = X_ts.transpose(\"vertex\", \"chromo\", \"time\")\n",
    "X_ts = X_ts.assign_coords(is_brain=('vertex', Adot.is_brain.values))\n",
    "\n",
    "scl = np.percentile(np.abs(X_ts.sel(chromo='HbO').values.reshape(-1)),99)\n",
    "clim = (-scl,scl)\n",
    "\n",
    "\n",
    "image_recon_multi_view(\n",
    "    X_ts,  # time series data; can be 2D (static) or 3D (dynamic)\n",
    "    head,\n",
    "    cmap='seismic',\n",
    "    clim=clim,\n",
    "    view_type='hbo_scalp',\n",
    "    title_str='HbO / uM',\n",
    "    filename=filename_multiview_scalp,\n",
    "    SAVE=True,\n",
    "    time_range=(-5,30,0.5)*units.s,\n",
    "    fps=6,\n",
    "    geo3d_plot = geo3d_plot,\n",
    "    wdw_size = (1024, 768)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(f\"{filename_multiview_scalp}.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "### Parcel Space\n",
    "\n",
    "The Schaefer Atlas <cite data-cite=\"Schaefer2018\">(Schaefer, 2018)</cite> as implemented in Cedalion provides nearly 600 labels for different regions of the brain. Each vertex of the brain surface has its correspondng parcel label assigned as a coordinate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "head.brain.vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "Using parcel labels, vertices belonging to the same brain region can be easily grouped together with the `DataArray.groupby` function.\n",
    "\n",
    "To obtain the average hemodynamic response in a parcel, the baseline-subtraced concentration changes of all vertices in a parcel are averaged. As baseline the first sample is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract baseline\n",
    "baseline =  dC_brain.isel(reltime=0) # first sample along reltime dimension\n",
    "dC_brain_blsubtracted = dC_brain - baseline\n",
    "\n",
    "# average over parcels\n",
    "avg_HbO = dC_brain_blsubtracted.sel(chromo=\"HbO\").groupby('parcel').mean()\n",
    "\n",
    "avg_HbR = dC_brain_blsubtracted.sel(chromo=\"HbR\").groupby('parcel').mean()\n",
    "\n",
    "display(dC_brain_blsubtracted.rename(\"dC_brain_blsubtracted\"))\n",
    "display(avg_HbO.rename(\"avg_HbO\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "The montage in this dataset covers only parts of the head. Consequently, many brain regions lack significant signal coverage due to the absence of optodes. \n",
    "\n",
    "To focus on relevant regions, a subset of parcels from the somatosensory and motor regions in both hemispheres is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_parcels = [\n",
    "    \"SomMotA_1_LH\", \"SomMotA_3_LH\", \"SomMotA_4_LH\", \"SomMotA_5_LH\", \"SomMotA_9_LH\", \"SomMotA_10_LH\",\n",
    "    \"SomMotA_1_RH\", \"SomMotA_2_RH\", \"SomMotA_3_RH\", \"SomMotA_4_RH\", \"SomMotA_6_RH\", \"SomMotA_7_RH\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "The following plot visualizes the montage and the selected parcels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map parcel labels to colors\n",
    "parcel_colors = {\n",
    "    parcel: p.cm.jet(i / (len(selected_parcels) - 1))\n",
    "    for i, parcel in enumerate(selected_parcels)\n",
    "}\n",
    "\n",
    "# assign colors to vertices\n",
    "b = cdc.VTKSurface.from_trimeshsurface(head.brain)\n",
    "b = pv.wrap(b.mesh)\n",
    "b[\"parcels\"] = np.asarray([\n",
    "    parcel_colors.get(parcel, (0.8, 0.8, 0.8, .3))\n",
    "    for parcel in head.brain.vertices.parcel.values\n",
    "])\n",
    "\n",
    "plt = pv.Plotter()\n",
    "\n",
    "plt.add_mesh(\n",
    "    b,\n",
    "    scalars=\"parcels\",\n",
    "    rgb=True,\n",
    "    smooth_shading=False\n",
    ")\n",
    "\n",
    "cedalion.plots.plot_labeled_points(plt, geo3d_plot)\n",
    "\n",
    "legends = [a for a in parcel_colors.items()]\n",
    "plt.add_legend(labels= legends, face='o', size=(0.3,0.3))\n",
    "\n",
    "cog = head.brain.vertices.mean(\"label\")\n",
    "cog = cog.pint.dequantify().values\n",
    "\n",
    "plt.camera.position = cog + [0,0,400]\n",
    "plt.camera.focal_point = cog\n",
    "plt.camera.up = [0,1,0]\n",
    "plt.reset_camera()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "Plot averaged time traces in each parcel for the 'FTapping/Right' and 'FTapping/Left' conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = p.subplots(2,6, figsize=(20,5))\n",
    "ax = ax.flatten()\n",
    "for i_par, par in enumerate(selected_parcels):\n",
    "    ax[i_par].plot(avg_HbO.sel(parcel = par, trial_type = \"FTapping/Right\").reltime, avg_HbO.sel(parcel = par, trial_type = \"FTapping/Right\").values, \"r\", lw=2, ls='-')\n",
    "    ax[i_par].plot(avg_HbR.sel(parcel = par, trial_type = \"FTapping/Right\").reltime, avg_HbR.sel(parcel = par, trial_type = \"FTapping/Right\").values, \"b\", lw=2, ls='-')\n",
    "\n",
    "    ax[i_par].grid(1)\n",
    "    ax[i_par].set_title(par, color=parcel_colors[par])\n",
    "    ax[i_par].set_ylim(-.05, .2)\n",
    "\n",
    "p.suptitle(\"Parcellations: HbO: r | HbR: b | FTapping/Right\", y=1)\n",
    "p.tight_layout()\n",
    "\n",
    "f,ax = p.subplots(2,6, figsize=(20,5))\n",
    "ax = ax.flatten()\n",
    "for i_par, par in enumerate(selected_parcels):\n",
    "    ax[i_par].plot(avg_HbO.sel(parcel = par, trial_type = \"FTapping/Left\").reltime, avg_HbO.sel(parcel = par, trial_type = \"FTapping/Left\").values, \"r\", lw=2, ls='-')\n",
    "    ax[i_par].plot(avg_HbR.sel(parcel = par, trial_type = \"FTapping/Left\").reltime, avg_HbR.sel(parcel = par, trial_type = \"FTapping/Left\").values, \"b\", lw=2, ls='-')\n",
    "\n",
    "    ax[i_par].grid(1)\n",
    "    ax[i_par].set_title(par, color=parcel_colors[par])\n",
    "    ax[i_par].set_ylim(-.05, .2)\n",
    "\n",
    "p.suptitle(\"Parcellations: HbO: r | HbR: b | FTapping/Left\", y=1)\n",
    "p.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cedalion_v25.1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
