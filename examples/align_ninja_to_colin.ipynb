{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0cdf1b6",
   "metadata": {},
   "source": [
    "# NinjaCap-wholeHeadHD-probe registration to Colin27\n",
    "This example jupyter notebook shows how to align the optodes of the NinjaCap-wholeHeadHD to the Colin27 head model.\n",
    "Thorough coregistration is the foundation of every data analysis using head models. \n",
    "\n",
    "Currently, `cedaÄºion` offers a simple registration method, which finds an affine transformation (scaling, rotating, translating) that matches the landmark positions of the head model and their digitized counterparts (probe data). Afterward, optodes are snapped to the nearest vertex on the scalp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f6968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "#pv.set_jupyter_backend('html')\n",
    "pv.set_jupyter_backend('static')\n",
    "#pv.OFF_SCREEN=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44081e4b-6981-456f-8394-2db713b6a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, xarray as xr\n",
    "\n",
    "import cedalion\n",
    "import cedalion.io as cio\n",
    "import cedalion.datasets\n",
    "import cedalion.imagereco.forward_model as fw\n",
    "import cedalion.geometry.registration as cgeoreg\n",
    "import cedalion.geometry.landmarks as cgeolm\n",
    "import cedalion.plots as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd2de1-f996-49ed-bf04-21dc3ea0f5ab",
   "metadata": {},
   "source": [
    "## Load segmented MRI scan\n",
    "\n",
    "For this example use a segmentation of the Colin27 average brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd860c3-06ee-4362-9f4d-55831dd5c8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEG_DATADIR, mask_files, landmarks_file = cedalion.datasets.get_colin27_segmentation()\n",
    "masks, t_ijk2ras = cedalion.io.read_segmentation_masks(SEG_DATADIR, mask_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b45f6-ef25-47e5-91a8-265ec54de11f",
   "metadata": {},
   "source": [
    "Construct Colin27 headmodel from segmentation masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2265831b-341a-4026-9320-c1d7870ad617",
   "metadata": {},
   "outputs": [],
   "source": [
    "colin = fw.TwoSurfaceHeadModel.from_surfaces(\n",
    "    segmentation_dir=SEG_DATADIR,\n",
    "    mask_files = mask_files,\n",
    "    brain_surface_file= os.path.join(SEG_DATADIR, \"mask_brain.obj\"),\n",
    "    scalp_surface_file= os.path.join(SEG_DATADIR, \"mask_scalp.obj\"),\n",
    "    landmarks_ras_file=landmarks_file,\n",
    "    smoothing=0.5,\n",
    "    fill_holes=True,\n",
    ")\n",
    "colin.scalp.units = cedalion.units.mm\n",
    "colin.brain.units = cedalion.units.mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88fca38-2129-4aed-91b7-25082cccc868",
   "metadata": {},
   "source": [
    "## Compute EEG's 10-10 system landmarks of Colin27 for optode coregistration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d62605-27f2-40b7-b3ec-f1c2c259ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the 10-10 system landmarks from the fiducials and the scalp using cedalions LandmarksBuilder1010\n",
    "scalp_surface = colin.scalp\n",
    "\n",
    "# Align fiducials to head coordinate system\n",
    "fiducials_ras = cio.read_mrk_json(os.path.join(SEG_DATADIR, landmarks_file), crs=\"aligned\")\n",
    "fiducials_ijk = fiducials_ras.points.apply_transform(np.linalg.pinv(t_ijk2ras))\n",
    "# Compute landmarks by EEG's 1010 system rules\n",
    "lmbuilder = cgeolm.LandmarksBuilder1010(scalp_surface, fiducials_ijk)\n",
    "all_landmarks = lmbuilder.build()\n",
    "lmbuilder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf24a64-bd3d-4356-9259-d41ba973cea8",
   "metadata": {},
   "source": [
    "## Load NinjaCap data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292bb1b9-a872-4aae-8b87-ad0f517475e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ninjacap_optodes, ninjacap_landmarks, meas_list = cedalion.datasets.get_ninja_cap_probe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d13113-ef9d-40cf-8117-20babffd12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handpick or load handpicked fiducials from file\n",
    "fiducials_ras = cio.read_mrk_json(os.path.join(SEG_DATADIR, landmarks_file), crs=\"aligned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dbc674-2918-41e5-8c7a-dc3ba163b021",
   "metadata": {},
   "source": [
    "## Construct transform from matching landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d460c-5cdc-4a27-8bc4-699cde5c3288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individial landmarks\n",
    "individual_ref_pos = np.array(all_landmarks) \n",
    "individual_ref_labels = [lab.item() for lab in all_landmarks.label] \n",
    "\n",
    "# Load ninja cap data\n",
    "ninjacap_optodes, ninjacap_landmarks, meas_list = cedalion.datasets.get_ninja_cap_probe() \n",
    "ninja_ref_pos = list(np.array(ninjacap_landmarks.values))\n",
    "ninja_ref_labels = list(np.array(ninjacap_landmarks.label))\n",
    "\n",
    "# Construct transform from intersection\n",
    "intersection = list(set(ninja_ref_labels) & set(individual_ref_labels)) \n",
    "individual_ref_pos = [individual_ref_pos[individual_ref_labels.index(intsct)] for intsct in intersection]\n",
    "ninja_ref_pos = [ninja_ref_pos[ninja_ref_labels.index(intsct)] for intsct in intersection]\n",
    "print(\"%d Landmarks used for co-registration:\\n\" % len(intersection), intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e8ac6-bc48-44bd-ae9c-406a0552656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This transform is somehow not working: I havn't figured out why yet\n",
    "\"\"\"\n",
    "# Individial landmarks\n",
    "individual_ref_pos = all_landmarks\n",
    "# the landmarks are in Colins current coordinate system\n",
    "individual_ref_pos = individual_ref_pos.rename({individual_ref_pos.points.crs: colin.scalp.crs})\n",
    "individual_ref_labels = [lab.item() for lab in all_landmarks.label] \n",
    "\n",
    "# Load ninja cap data\n",
    "ninjacap_optodes, ninjacap_landmarks, meas_list = cedalion.datasets.get_ninja_cap_probe() \n",
    "ninja_ref_pos = ninjacap_landmarks\n",
    "ninja_ref_labels = list(np.array(ninjacap_landmarks.label))\n",
    "\n",
    "# Construct transform from intersection\n",
    "intersection = list(set(ninja_ref_labels) & set(individual_ref_labels))\n",
    "print(\"%d Landmarks used for co-registration:\\n\" % len(intersection), intersection)\n",
    "\n",
    "individual_ref_pos = individual_ref_pos.sel(label=intersection)\n",
    "ninja_ref_pos = ninja_ref_pos.sel(label=intersection)\n",
    "ninja_ref_pos = ninja_ref_pos.pint.quantify(cedalion.units.mm)\n",
    "\n",
    "T = cgeoreg.register_trans_rot_isoscale(individual_ref_pos, ninja_ref_pos)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3291392b-48bb-4688-b385-af80c6993e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative, non-cedalion, implementation from atlasviewer\n",
    "def gen_xform_from_pts(p1, p2):\n",
    "    \"\"\"\n",
    "    given two sets of points, p1 and p2 in n dimensions,\n",
    "    find the n-dims affine transformation matrix t, from p1 to p2.\n",
    "\n",
    "    Source: https://github.com/bunpc/atlasviewer/blob/71fc98ec8ca54783378310304113e825bbcd476a/utils/gen_xform_from_pts.m#l4\n",
    "    \n",
    "    parameters:\n",
    "    p1 : ndarray\n",
    "        an array of shape (p, n) representing the first set of points.\n",
    "    p2 : ndarray\n",
    "        an array of shape (p, n) representing the second set of points.\n",
    "\n",
    "    returns:\n",
    "    t : ndarray\n",
    "        the (n+1, n+1) affine transformation matrix.\n",
    "    \"\"\"\n",
    "    p1, p2 = np.array(p1), np.array(p2)\n",
    "    p = p1.shape[0]\n",
    "    q = p2.shape[0]\n",
    "    m = p1.shape[1]\n",
    "    n = p2.shape[1]\n",
    "    \n",
    "    if p != q:\n",
    "        raise valueerror('number of points for p1 and p2 must be the same')\n",
    "    \n",
    "    if m != n:\n",
    "        raise valueerror('number of dimensions for p1 and p2 must be the same')\n",
    "    \n",
    "    if p < n:\n",
    "        raise valueerror(f'cannot solve transformation with fewer anchor points ({p}) than dimensions ({n}).')\n",
    "    \n",
    "    t = np.eye(n + 1)\n",
    "    a = np.hstack((p1, np.ones((p, 1))))\n",
    "    \n",
    "    for ii in range(n):\n",
    "        x = np.linalg.pinv(a) @ p2[:, ii]\n",
    "        t[ii, :] = x\n",
    "        \n",
    "    return t\n",
    "\n",
    "\n",
    "T = gen_xform_from_pts(ninja_ref_pos, individual_ref_pos); # get affine  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e34e31-da69-4b17-b710-1b5d08d803a4",
   "metadata": {},
   "source": [
    "## Apply transform and snap optodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749dc0ff-920b-44ed-824d-0e1f9349c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transform\n",
    "ninja_aligned = ninjacap_optodes.points.apply_transform(T)\n",
    "if isinstance(T, np.ndarray):\n",
    "    ninja_aligned = ninja_aligned.rename({ninja_aligned.points.crs: colin.scalp.crs})\n",
    "plt = pv.Plotter()\n",
    "cedalion.plots.plot_surface(plt, colin.scalp, opacity=0.1)\n",
    "cedalion.plots.plot_labeled_points(plt, ninja_aligned)\n",
    "plt.show()\n",
    "\n",
    "# Snap to surface\n",
    "ninja_snapped_aligned = colin.scalp.snap(ninja_aligned)\n",
    "# Plot\n",
    "plt = pv.Plotter()\n",
    "cedalion.plots.plot_surface(plt, colin.scalp)\n",
    "cedalion.plots.plot_labeled_points(plt, ninja_snapped_aligned)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7128e06-3558-4103-8628-e7a46772429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct forward model\n",
    "fwm = cedalion.imagereco.forward_model.ForwardModel(colin, ninja_snapped_aligned, meas_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048fcde-3b91-45b3-bf78-688d27360ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
